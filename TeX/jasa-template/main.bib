%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Galyardt at 2014-08-21 22:49:39 -0400 


%% Saved with string encoding Unicode (UTF-8) 

@article{GramacyApley,
author = {Robert B. Gramacy and Daniel W. Apley},
title = {Local Gaussian Process Approximation for Large Computer Experiments},
journal = {Journal of Computational and Graphical Statistics},
volume = 24,
number = 2,
pages = {561-578},
year  = 2015,
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2014.914442},
URL = { 
        https://doi.org/10.1080/10618600.2014.914442
},
eprint = { 
        https://doi.org/10.1080/10618600.2014.914442
    
}

}





@Article{Craven1978,
author="Craven, Peter
and Wahba, Grace",
title="Smoothing noisy data with spline functions",
journal="Numerische Mathematik",
year="1978",
month="Dec",
day="01",
volume="31",
number="4",
pages="377--403",
abstract="Smoothing splines are well known to provide nice curves
                  which smooth discrete, noisy data. We obtain a
                  practical, effective method for estimating the
                  optimum amount of smoothing from the
                  data. Derivatives can be estimated from the data by
                  differentiating the resulting (nearly) optimally
                  smoothed spline.",
issn="0945-3245",
doi="10.1007/BF01404567",
url="https://doi.org/10.1007/BF01404567"
}

@book{Wahba1990,
title = {Spline Models for Observational Data},
publish = {SIAM},
author = {Wahba, G.},
year = {1990},
doi = {10.1137/1.9781611970128.fm},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611970128.fm}
}

@article{Wood2001,
author = {Wood, Simon N.},
title = {Thin plate regression splines},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {65},
number = {1},
pages = {95-114},
keywords = {Generalized additive model, Regression spline, Thin plate spline},
doi = {10.1111/1467-9868.00374},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00374},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00374},
abstract = {Summary. I discuss the production of low rank smoothers
                  for d ≥ 1 dimensional data, which can be fitted by
                  regression or penalized regression methods. The
                  smoothers are constructed by a simple transformation
                  and truncation of the basis that arises from the
                  solution of the thin plate spline smoothing problem
                  and are optimal in the sense that the truncation is
                  designed to result in the minimum possible
                  perturbation of the thin plate spline smoothing
                  problem given the dimension of the basis used to
                  construct the smoother. By making use of Lanczos
                  iteration the basis change and truncation are
                  computationally efficient. The smoothers allow the
                  use of approximate thin plate spline models with
                  large data sets, avoid the problems that are
                  associated with ‘knot placement’ that usually
                  complicate modelling with regression splines or
                  penalized regression splines, provide a sensible way
                  of modelling interaction terms in generalized
                  additive models, provide low rank approximations to
                  generalized smoothing spline models, appropriate for
                  use with large data sets, provide a means for
                  incorporating smooth functions of more than one
                  variable into non-linear models and improve the
                  computational efficiency of penalized likelihood
                  models incorporating thin plate splines. Given that
                  the approach produces spline-like models with a
                  sparse basis, it also provides a natural way of
                  incorporating unpenalized spline-like terms in
                  linear and generalized linear models, and these can
                  be treated just like any other model terms from the
                  point of view of model selection, inference and
                  diagnostics.},
year = {2003}
}

@misc{Wood2017,
title={Generalized Additive Models: An Introduction with R, Second
                  Edition},
url={https://www.taylorfrancis.com/books/9781498728348},
journal={Taylor & Francis},                  
publisher={Routledge},
author={Wood, Simon N.},
year={2017},
month={May}} 

@article{DupuisRobert,
title = "Variable selection in qualitative models via an entropic
                  explanatory power",
journal = "Journal of Statistical Planning and Inference",
volume = "111",
number = "1",
pages = "77 - 94",
year = "2003",
note = "Special issue I: Model Selection, Model Diagnostics, Empirical
                  {Bayes} and Hierarchical {Bayes}",
issn = "0378-3758",
doi = "https://doi.org/10.1016/S0378-3758(02)00286-0",
url =
                  "http://www.sciencedirect.com/science/article/pii/S0378375802002860",
author = "J\'{e}rome A. Dupuis and Christian P. Robert",
keywords = "Additivity property, Entropy, Kullback–Leibler distance,
                  Logit model, Transitivity",
abstract = "The variable selection method proposed in the paper is
                  based on the evaluation of the Kullback–Leibler
                  distance between the full (or encompassing) model
                  and its submodels. The Bayesian implementation of
                  the method does not require a separate prior
                  modeling on the submodels since the corresponding
                  parameters for the submodels are defined as the
                  Kullback–Leibler projections of the full model
                  parameters. The result of the selection procedure is
                  the submodel with the smallest number of covariates
                  which is at an acceptable distance of the full
                  model. We introduce the notion of explanatory power
                  of a model and scale the maximal acceptable distance
                  in terms of the explanatory power of the full
                  model. Moreover, an additivity property between
                  embedded submodels shows that our selection
                  procedure is equivalent to select the submodel with
                  the smallest number of covariates which has a
                  sufficient explanatory power. We illustrate the
                  performances of this method on a breast cancer
                  dataset"
}

@article{GoutisRobert,
    author = {Goutis, Constantinos and Robert, Christian P.},
    title = "{Model choice in generalised linear models: A Bayesian
                  approach via Kullback-Leibler projections}",
    journal = {Biometrika},
    volume = 85,
    number = 1,
    pages = {29-37},
    year = 1998,
    month = 03,
    abstract = "{We propose a general Bayesian method of comparing
                  models. The approach is based on the
                  Kullback-Leibler distance between two families of
                  models, one nested within the other. For each
                  parameter value of a full model, we compute the
                  projection of the model to the restricted parameter
                  space and the corresponding minimum distance. From
                  the posterior distribution of the minimum distance,
                  we can judge whether or not a more parsimonious
                  model is appropriate. We show how the projection
                  method can be implemented for generalised linear
                  model selection and we propose some Markov chain
                  Monte Carlo algorithms for its practical
                  implementation in less tractable cases. We
                  illustrate the method with examples.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/85.1.29},
    url = {https://dx.doi.org/10.1093/biomet/85.1.29},
    eprint =
                  {http://oup.prod.sis.lan/biomet/article-pdf/85/1/29/656553/85-1-29.pdf},
}



@Article{Piironen2017,
author="Piironen, Juho
and Vehtari, Aki",
title="Comparison of {Bayesian} predictive methods for model selection",
journal="Statistics and Computing",
year="2017",
month="May",
day="01",
volume="27",
number="3",
pages="711--735",
abstract="The goal of this paper is to compare several widely used
                  Bayesian model selection methods in practical model
                  selection problems, highlight their differences and
                  give recommendations about the preferred
                  approaches. We focus on the variable subset
                  selection for regression and classification and
                  perform several numerical experiments using both
                  simulated and real world data. The results show that
                  the optimization of a utility estimate such as the
                  cross-validation (CV) score is liable to finding
                  overfitted models due to relatively high variance in
                  the utility estimates when the data is scarce. This
                  can also lead to substantial selection induced bias
                  and optimism in the performance evaluation for the
                  selected model. From a predictive viewpoint, best
                  results are obtained by accounting for model
                  uncertainty by forming the full encompassing model,
                  such as the Bayesian model averaging solution over
                  the candidate models. If the encompassing model is
                  too complex, it can be robustly simplified by the
                  projection method, in which the information of the
                  full model is projected onto the submodels. This
                  approach is substantially less prone to overfitting
                  than selection based on CV-score. Overall, the
                  projection method appears to outperform also the
                  maximum a posteriori model and the selection of the
                  most probable variables. The study also demonstrates
                  that the model selection can greatly benefit from
                  using cross-validation outside the searching process
                  both for guiding the model size selection and
                  assessing the predictive performance of the finally
                  selected model.",
issn="1573-1375",
doi="10.1007/s11222-016-9649-y",
url="https://doi.org/10.1007/s11222-016-9649-y"
}


@article{Vehtari2012,
author = "Vehtari, Aki and Ojanen, Janne",
doi = "10.1214/12-SS102",
fjournal = "Statistics Surveys",
journal = "Statist. Surv.",
pages = "142--228",
publisher = "The American Statistical Association, the Bernoulli
                  Society, the Institute of Mathematical Statistics,
                  and the Statistical Society of Canada",
title = "A survey of {Bayesian} predictive methods for model assessment,
                  selection and comparison",
url = "https://doi.org/10.1214/12-SS102",
volume = "6",
year = "2012"
}



@article{ClydeGeorge,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/4144374},
 abstract = {The evolution of Bayesian approaches for model
                  uncertainty over the past decade has been
                  remarkable. Catalyzed by advances in methods and
                  technology for posterior computation, the scope of
                  these methods has widened substantially. Major
                  thrusts of these developments have included new
                  methods for semiautomatic prior specification and
                  posterior exploration. To illustrate key aspects of
                  this evolution, the highlights of some of these
                  developments are described.},
 author = {Merlise Clyde and Edward I. George},
 journal = {Statistical Science},
 number = {1},
 pages = {81--94},
 publisher = {Institute of Mathematical Statistics},
 title = {Model Uncertainty},
 volume = {19},
 year = {2004}
}


@article{NottLeng,
title = "Bayesian projection approaches to variable selection in
                  generalized linear models",
journal = "Computational Statistics & Data Analysis",
volume = "54",
number = "12",
pages = "3227 - 3241",
year = "2010",
issn = "0167-9473",
doi = "https://doi.org/10.1016/j.csda.2010.01.036",
url =
                  "http://www.sciencedirect.com/science/article/pii/S016794731000054X",
author = "David J. Nott and Chenlei Leng",
keywords = "Bayesian variable selection, Kullback–Leibler projection,
                  Lasso, Non-negative garotte, Preconditioning",
abstract = "A Bayesian approach to variable selection which is based
                  on the expected Kullback–Leibler divergence between
                  the full model and its projection onto a submodel
                  has recently been suggested in the literature. For
                  generalized linear models an extension of this idea
                  is proposed by considering projections onto
                  subspaces defined via some form of L1 constraint on
                  the parameter in the full model. This leads to
                  Bayesian model selection approaches related to the
                  lasso. In the posterior distribution of the
                  projection there is positive probability that some
                  components are exactly zero and the posterior
                  distribution on the model space induced by the
                  projection allows exploration of model
                  uncertainty. Use of the approach in structured
                  variable selection problems such as ANOVA models is
                  also considered, where it is desired to incorporate
                  main effects in the presence of
                  interactions. Projections related to the
                  non-negative garotte are able to respect the
                  hierarchical constraints. A consistency result is
                  given concerning the posterior distribution on the
                  model induced by the projection, showing that for
                  some projections related to the adaptive lasso and
                  non-negative garotte the posterior distribution
                  concentrates on the true model asymptotically."
}

@book{BayesianChoice,
author = {Robert, Christian},
year = {2001},
month = {01},
pages = {},
title = {The Bayesian Choice},
publisher = {Springer},
doi = {10.1007/978-1-4757-4314-2}
}

@article{PuelzSUR,
author = "Puelz, David and Hahn, P. Richard and Carvalho, Carlos M.",
doi = "10.1214/17-BA1053",
fjournal = "Bayesian Analysis",
journal = "Bayesian Anal.",
month = "12",
number = "4",
pages = "969--989",
publisher = "International Society for Bayesian Analysis",
title = "Variable Selection in Seemingly Unrelated Regressions with
                  Random Predictors",
url = "https://doi.org/10.1214/17-BA1053",
volume = "12",
year = "2017"
}



@ARTICLE{Piironenetal2018,
       author = {{Piironen}, Juho and {Paasiniemi}, Markus and
                  {Vehtari}, Aki},
        title = "{Projective Inference in High-dimensional Problems:
                  Prediction and Feature Selection}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science -
                  Machine Learning},
         year = 2018,
        month = Oct,
          eid = {arXiv:1810.02406},
        pages = {arXiv:1810.02406},
archivePrefix = {arXiv},
       eprint = {1810.02406},
 primaryClass = {stat.ML},
       adsurl =
                  {https://ui.adsabs.harvard.edu/\#abs/2018arXiv181002406P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{Friedman2001,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2699986},
 abstract = {Function estimation/approximation is viewed from the
                  perspective of numerical optimization in function
                  space, rather than parameter space. A connection is
                  made between stagewise additive expansions and
                  steepest-descent minimization. A general gradient
                  descent "boosting" paradigm is developed for
                  additive expansions based on any fitting
                  criterion. Specific algorithms are presented for
                  least-squares, least absolute deviation, and Huber-M
                  loss functions for regression, and multiclass
                  logistic likelihood for classification. Special
                  enhancements are derived for the particular case
                  where the individual additive components are
                  regression trees, and tools for interpreting such
                  "TreeBoost" models are presented. Gradient boosting
                  of regression trees produces competitive, highly
                  robust, interpretable procedures for both regression
                  and classification, especially appropriate for
                  mining less than clean data. Connections between
                  this approach and the boosting methods of Freund and
                  Shapire and Friedman, Hastie and Tibshirani are
                  discussed.},
 author = {Jerome H. Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189--1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 volume = {29},
 year = {2001}
}

                  
@article{RIClinear,
author = "Hahn, P. Richard and Carvalho, Carlos M. and Puelz, David
                  and He, Jingyu",
doi = "10.1214/16-BA1044",
fjournal = "Bayesian Analysis",
journal = "Bayesian Anal.",
month = "03",
number = "1",
pages = "163--182",
publisher = "International Society for Bayesian Analysis",
title = "Regularization and Confounding in Linear Regression for
                  Treatment Effect Estimation",
url = "https://doi.org/10.1214/16-BA1044",
volume = "13",
year = "2018"
}



@article{lee2016,
author = "Lee, Jason D. and Sun, Dennis L. and Sun, Yuekai and Taylor,
                  Jonathan E.",
doi = "10.1214/15-AOS1371",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "06",
number = "3",
pages = "907--927",
publisher = "The Institute of Mathematical Statistics",
title = "Exact post-selection inference, with application to the
                  lasso",
url = "https://doi.org/10.1214/15-AOS1371",
volume = "44",
year = "2016"
}


@article{adaptivelasso,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/27639762},
 abstract = {The lasso is a popular technique for simultaneous
                  estimation and variable selection. Lasso variable
                  selection has been shown to be consistent under
                  certain conditions. In this work we derive a
                  necessary condition for the lasso variable selection
                  to be consistent. Consequently, there exist certain
                  scenarios where the lasso is inconsistent for
                  variable selection. We then propose a new version of
                  the lasso, called the adaptive lasso, where adaptive
                  weights are used for penalizing different
                  coefficients in the l1 penalty. We show that the
                  adaptive lasso enjoys the oracle properties; namely,
                  it performs as well as if the true underlying model
                  were given in advance. Similar to the lasso, the
                  adaptive lasso is shown to be near-minimax
                  optimal. Furthermore, the adaptive lasso can be
                  solved by the same efficient algorithm for solving
                  the lasso. We also discuss the extension of the
                  adaptive lasso in generalized linear models and show
                  that the oracle properties still hold under mild
                  regularity conditions. As a byproduct of our theory,
                  the nonnegative garotte is shown to be consistent
                  for variable selection.},
 author = {Hui Zou},
 journal = {Journal of the American Statistical Association},
 number = {476},
 pages = {1418--1429},
 publisher = {[American Statistical Association, Taylor & Francis,
                  Ltd.]},
 title = {The Adaptive Lasso and Its Oracle Properties},
 volume = {101},
 year = {2006}
}

@article{einstein,
    author = "Albert Einstein",
    title = "{Zur Elektrodynamik bewegter K{\"o}rper}. ({German})
    [{On} the electrodynamics of moving bodies]",
    journal = "Annalen der Physik",
    volume = "322",
    number = "10",
    pages = "891--921",
    year = "1905",
    DOI = "http://dx.doi.org/10.1002/andp.19053221004",
    keywords = "physics"
}

@article{DSS,
author = {P. Richard Hahn and Carlos M. Carvalho},
title = {Decoupling Shrinkage and Selection in {Bayesian} Linear
                  Models: A Posterior Summary Perspective},
journal = {Journal of the American Statistical Association},
volume = {110},
number = {509},
pages = {435-448},
year = {2015},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2014.993077},
URL = {https://doi.org/10.1080/01621459.2014.993077},
eprint = {https://doi.org/10.1080/01621459.2014.993077}
}

@inproceedings{lime,
 author = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
 title = { "{Why} Should {I} Trust You?": Explaining the Predictions
                  of Any Classifier },
 booktitle = {Knowledge Discovery and Data Mining (KDD)},
 year = {2016}
}



@incollection{Galyardt14mmm,
	Author = {April Galyardt},
	Booktitle = {Handbook of Mixed Membership Models},
	Date-Added = {2014-08-21 21:18:27 +0000},
	Date-Modified = {2014-08-21 21:18:27 +0000},
	Editor = {Edoardo M. Airoldi and David Blei and Erosheva, Elena and Fienberg, Stephen E.},
	Publisher = {Chapman and Hall},
	Title = {Interpreting Mixed Membership Models: Implications of Erosheva's Representation Theorem},
	Year = {2014},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QMi4uLy4uL1BhcGVyLU1NIGNoYXB0ZXIvR2FseWFyZHQtY2hhcHRlci03LjguMTMucGRm0hcLGBlXTlMuZGF0YU8RAeIAAAAAAeIAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMm3i1VIKwAAAJ3/thtHYWx5YXJkdC1jaGFwdGVyLTcuOC4xMy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAu7Y5zgBejQAAAAAAAAAAAAIAAgAACSAAAAAAAAAAAAAAAAAAAAAQUGFwZXItTU0gY2hhcHRlcgAQAAgAAMm3w5UAAAARAAgAAM4Als0AAAABABAAnf+2ABefWwANUdgAAJg6AAIAVU1hY2ludG9zaCBIRDpVc2VyczoAYWdhbHlhcmR0OgBEcm9wYm94OgBQYXBlci1NTSBjaGFwdGVyOgBHYWx5YXJkdC1jaGFwdGVyLTcuOC4xMy5wZGYAAA4AOAAbAEcAYQBsAHkAYQByAGQAdAAtAGMAaABhAHAAdABlAHIALQA3AC4AOAAuADEAMwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIARFVzZXJzL2FnYWx5YXJkdC9Ecm9wYm94L1BhcGVyLU1NIGNoYXB0ZXIvR2FseWFyZHQtY2hhcHRlci03LjguMTMucGRmABMAAS8AABUAAgAQ//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4AwwDIANACtgK4Ar0CyALRAt8C4wLqAvMC+AMFAwgDGgMdAyIAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADJA==}}

@phdthesis{Galyardt12dis,
	Address = {Pittsburgh, PA 15213},
	Author = {April Galyardt},
	Date-Added = {2014-08-21 21:17:56 +0000},
	Date-Modified = {2014-08-22 02:49:38 +0000},
	Keywords = {mixed membership, Latent Dirichlet Allocation, cognitive diagnosis models, numerical estimation, psychometrics},
	Month = {July},
	School = {Carnegie Mellon University},
	Title = {Mixed Membership Distributions with Applications to Modeling Multiple Strategy Usage},
	Year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QRC4uLy4uLy4uL0RvY3VtZW50cy9QdWJsaWNhdGlvbnMvMjAxMi1HYWx5YXJkdC1EaXNzZXJ0YXRpb24tRmluYWwucGRm0hcLGBlXTlMuZGF0YU8RAfoAAAAAAfoAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMtppx9IKwAAAAuN+h8yMDEyLUdhbHlhcmR0LURpc3NlciMyNjgyNTgucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJoJYzMbb7wAAAAAAAAAAAAMAAwAACSAAAAAAAAAAAAAAAAAAAAAMUHVibGljYXRpb25zABAACAAAy2ntbwAAABEACAAAzMciPwAAAAEAEAALjfoABcifAAXIngAAuVcAAgBXTWFjaW50b3NoIEhEOlVzZXJzOgBhZ2FseWFyZHQ6AERvY3VtZW50czoAUHVibGljYXRpb25zOgAyMDEyLUdhbHlhcmR0LURpc3NlciMyNjgyNTgucGRmAAAOAEoAJAAyADAAMQAyAC0ARwBhAGwAeQBhAHIAZAB0AC0ARABpAHMAcwBlAHIAdABhAHQAaQBvAG4ALQBGAGkAbgBhAGwALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAEtVc2Vycy9hZ2FseWFyZHQvRG9jdW1lbnRzL1B1YmxpY2F0aW9ucy8yMDEyLUdhbHlhcmR0LURpc3NlcnRhdGlvbi1GaW5hbC5wZGYAABMAAS8AABUAAgAQ//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4A1QDaAOIC4ALiAucC8gL7AwkDDQMUAx0DIgMvAzIDRANHA0wAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADTg==}}

@conference{Galyardt13smmc,
	Address = {Arnhem, Netherlands},
	Author = {April Galyardt and Ilya Goldin},
	Booktitle = {International Meeting of the Psychometric Society},
	Date-Added = {2014-08-21 21:17:50 +0000},
	Date-Modified = {2014-08-21 21:17:50 +0000},
	Keywords = {conf},
	Title = {Modeling Student Metacognitive Strategies in a Intelligent Tutoring System},
	Year = {2013}}

@article{Campbell02,
	Author = {Jamie I.D. Campbell and Shauna Austin},
	Date-Added = {2014-08-21 21:12:16 +0000},
	Date-Modified = {2014-08-21 21:13:22 +0000},
	Journal = {Memory \& Cognition},
	Keywords = {strategies, addition, response time},
	Number = {6},
	Pages = {988-994},
	Title = {Effects of response time deadlines on adults' strategy choices for simple addition},
	Volume = {30},
	Year = {2002}}

@article{Schubert13,
	Author = {Schubert, Christiane C and Denmark, T Kent and Crandall, Beth and Grome, Anna and Pappas, James},
	Date-Added = {2014-08-21 21:00:46 +0000},
	Date-Modified = {2014-08-21 21:00:46 +0000},
	Journal = {Annals of emergency medicine},
	Keywords = {expert/novice differences},
	Number = {1},
	Pages = {96--109},
	Publisher = {Elsevier},
	Title = {Characterizing novice-expert differences in macrocognition: an exploratory study of cognitive work in the emergency department},
	Volume = {61},
	Year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QYS4uLy4uL1BhcGVyLVdoYXQncyBhIFN0cmF0ZWd5L1BhcGVycy1TdHJhdGVnaWVzL1JlYWQvU2NodWJlcnQgZXRhbCAoMjAxMikgLUVtZXJnZW5jeSBNZWRpY2luZS5wZGbSFwsYGVdOUy5kYXRhTxECVAAAAAACVAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAybeLVUgrAAACeKY6H1NjaHViZXJ0IGV0YWwgKDIwMTIjMjc4QTdCNi5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJ4p7bP6W1eAAAAAAAAAAAAAgAEAAAJIAAAAAAAAAAAAAAAAAAAAARSZWFkABAACAAAybfDlQAAABEACAAAz+mlngAAAAEAGAJ4pjoCeKY4ANZobAAXn1sADVHYAACYOgACAHlNYWNpbnRvc2ggSEQ6VXNlcnM6AGFnYWx5YXJkdDoARHJvcGJveDoAUGFwZXItV2hhdCdzIGEgU3RyYXRlZ3k6AFBhcGVycy1TdHJhdGVnaWVzOgBSZWFkOgBTY2h1YmVydCBldGFsICgyMDEyIzI3OEE3QjYucGRmAAAOAFoALABTAGMAaAB1AGIAZQByAHQAIABlAHQAYQBsACAAKAAyADAAMQAyACkAIAAtAEUAbQBlAHIAZwBlAG4AYwB5ACAATQBlAGQAaQBjAGkAbgBlAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBzVXNlcnMvYWdhbHlhcmR0L0Ryb3Bib3gvUGFwZXItV2hhdCdzIGEgU3RyYXRlZ3kvUGFwZXJzLVN0cmF0ZWdpZXMvUmVhZC9TY2h1YmVydCBldGFsICgyMDEyKSAtRW1lcmdlbmN5IE1lZGljaW5lLnBkZgAAEwABLwAAFQACABD//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgDyAPcA/wNXA1kDXgNpA3IDgAOEA4sDlAOZA6YDqQO7A74DwwAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAPF}}

@article{Siegler87,
	Author = {Robert S. Siegler},
	Date-Added = {2014-08-21 21:00:35 +0000},
	Date-Modified = {2014-08-21 21:00:35 +0000},
	Journal = {Journal of Experimental Psychology: General},
	Keywords = {Multiple strategy use; mathematics education; strategies; learning sciences; psychology},
	Number = {3},
	Pages = {250-264},
	Title = {The perils of averaging data over strategies: An example from children's addition},
	Volume = {116},
	Year = {1987},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8Qby4uLy4uLy4uL0RvY3VtZW50cy9Kb3VybmFsLU5ld3MtQXJ0aWNsZXMvTGVhcm5pbmcgU2NpZW5jZXMsIExBLCBFRE0vU2llZ2xlci04Ny1NdWx0aXBsZSBBZGRpdGlvbiBTdHJhdGVnaWVzLnBkZtIXCxgZV05TLmRhdGFPEQJoAAAAAAJoAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADLaacfSCsAAAALnbIfU2llZ2xlci04Ny1NdWx0aXBsZSBBI0I5RTE1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAueFcjZ9odQREYgAAAAAAADAAQAAAkgAAAAAAAAAAAAAAAAAAAAGkxlYXJuaW5nIFNjaWVuY2VzLCBMQSwgRURNABAACAAAy2ntbwAAABEACAAAyNouxwAAAAEAFAALnbIAC5wCAAXInwAFyJ4AALlXAAIAfE1hY2ludG9zaCBIRDpVc2VyczoAYWdhbHlhcmR0OgBEb2N1bWVudHM6AEpvdXJuYWwtTmV3cy1BcnRpY2xlczoATGVhcm5pbmcgU2NpZW5jZXMsIExBLCBFRE06AFNpZWdsZXItODctTXVsdGlwbGUgQSNCOUUxNS5wZGYADgBYACsAUwBpAGUAZwBsAGUAcgAtADgANwAtAE0AdQBsAHQAaQBwAGwAZQAgAEEAZABkAGkAdABpAG8AbgAgAFMAdAByAGEAdABlAGcAaQBlAHMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAHZVc2Vycy9hZ2FseWFyZHQvRG9jdW1lbnRzL0pvdXJuYWwtTmV3cy1BcnRpY2xlcy9MZWFybmluZyBTY2llbmNlcywgTEEsIEVETS9TaWVnbGVyLTg3LU11bHRpcGxlIEFkZGl0aW9uIFN0cmF0ZWdpZXMucGRmABMAAS8AABUAAgAQ//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4BAAEFAQ0DeQN7A4ADiwOUA6IDpgOtA7YDuwPIA8sD3QPgA+UAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAAD5w==}}

@article{Chi81,
	Author = {Chi, Michelene T.H. and Feltovich, Paul J. and Glaser, Robert},
	Date-Added = {2014-08-21 21:00:27 +0000},
	Date-Modified = {2014-08-21 21:00:27 +0000},
	Journal = {Cognitive science},
	Keywords = {expert/novice differences},
	Number = {2},
	Pages = {121--152},
	Publisher = {Wiley Online Library},
	Title = {Categorization and representation of physics problems by experts and novices},
	Volume = {5},
	Year = {1981},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QZy4uLy4uL1BhcGVyLVdoYXQncyBhIFN0cmF0ZWd5L1BhcGVycy1TdHJhdGVnaWVzL1JlYWQvQ2hpLCBGZWx0b3ZpY2gsIEdsYXNlciAoMTk4MSktcGh5c2ljcyBwcm9ibGVtcy5wZGbSFwsYGVdOUy5kYXRhTxECZgAAAAACZgACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAybeLVUgrAAACeKY6H0NoaSwgRmVsdG92aWNoLCBHbGEjMjc4QTdDQy5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJ4p8zP6ZB8AAAAAAAAAAAAAgAEAAAJIAAAAAAAAAAAAAAAAAAAAARSZWFkABAACAAAybfDlQAAABEACAAAz+nIvAAAAAEAGAJ4pjoCeKY4ANZobAAXn1sADVHYAACYOgACAHlNYWNpbnRvc2ggSEQ6VXNlcnM6AGFnYWx5YXJkdDoARHJvcGJveDoAUGFwZXItV2hhdCdzIGEgU3RyYXRlZ3k6AFBhcGVycy1TdHJhdGVnaWVzOgBSZWFkOgBDaGksIEZlbHRvdmljaCwgR2xhIzI3OEE3Q0MucGRmAAAOAGYAMgBDAGgAaQAsACAARgBlAGwAdABvAHYAaQBjAGgALAAgAEcAbABhAHMAZQByACAAKAAxADkAOAAxACkALQBwAGgAeQBzAGkAYwBzACAAcAByAG8AYgBsAGUAbQBzAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgB5VXNlcnMvYWdhbHlhcmR0L0Ryb3Bib3gvUGFwZXItV2hhdCdzIGEgU3RyYXRlZ3kvUGFwZXJzLVN0cmF0ZWdpZXMvUmVhZC9DaGksIEZlbHRvdmljaCwgR2xhc2VyICgxOTgxKS1waHlzaWNzIHByb2JsZW1zLnBkZgAAEwABLwAAFQACABD//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgD4AP0BBQNvA3EDdgOBA4oDmAOcA6MDrAOxA74DwQPTA9YD2wAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAPd}}

@incollection{Mislevy06Cog,
	Author = {Robert Mislevy},
	Booktitle = {Educational Assessment},
	Chapter = {8},
	Date-Added = {2014-08-21 20:58:59 +0000},
	Date-Modified = {2014-08-21 20:58:59 +0000},
	Editor = {Robert L. Brennan},
	Publisher = {American Council on Education and Praeger Publishers},
	Title = {Cognitive Psychology and Educational Assessment},
	Year = {2006},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QUy4uLy4uL1BhcGVyLVdoYXQncyBhIFN0cmF0ZWd5L1BhcGVycy1Nb2RlbHMvTWlzbGV2eSAoMjAwNiktQ29nIFBzeSAmIEFzc2Vzc21lbnQucGRm0hcLGBlXTlMuZGF0YU8RAjgAAAAAAjgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMm3i1VIKwAAAnimNh9NaXNsZXZ5ICgyMDA2KS1Db2cgIzI3QjBENEIucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACew1Lz+7ulQAAAAAAAAAAAAIAAwAACSAAAAAAAAAAAAAAAAAAAAANUGFwZXJzLU1vZGVscwAAEAAIAADJt8OVAAAAEQAIAADP7ybVAAAAAQAUAnimNgDWaGwAF59bAA1R2AAAmDoAAgBvTWFjaW50b3NoIEhEOlVzZXJzOgBhZ2FseWFyZHQ6AERyb3Bib3g6AFBhcGVyLVdoYXQncyBhIFN0cmF0ZWd5OgBQYXBlcnMtTW9kZWxzOgBNaXNsZXZ5ICgyMDA2KS1Db2cgIzI3QjBENEIucGRmAAAOAFAAJwBNAGkAcwBsAGUAdgB5ACAAKAAyADAAMAA2ACkALQBDAG8AZwAgAFAAcwB5ACAAJgAgAEEAcwBzAGUAcwBzAG0AZQBuAHQALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAGVVc2Vycy9hZ2FseWFyZHQvRHJvcGJveC9QYXBlci1XaGF0J3MgYSBTdHJhdGVneS9QYXBlcnMtTW9kZWxzL01pc2xldnkgKDIwMDYpLUNvZyBQc3kgJiBBc3Nlc3NtZW50LnBkZgAAEwABLwAAFQACABD//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgDkAOkA8QMtAy8DNAM/A0gDVgNaA2EDagNvA3wDfwORA5QDmQAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAOb}}

@article{Mislevy12,
	Author = {Robert J. Mislevy and John T. Behrens and Kristen E. {DiCerbo} and Roy Levy},
	Date-Added = {2014-03-27 15:22:33 +0000},
	Date-Modified = {2014-03-27 15:22:33 +0000},
	Journal = {Journal of Educational Data Mining},
	Number = {1},
	Title = {Design and Discovery in Educational Assessment: Evidence-Centered Design, Psychometrics, and Educational Data Mining},
	Volume = {4},
	Year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QWC4uLy4uL1ByaXZhdGUtRVJTSDg5OTAvUmVhZGluZ0xpc3QvQ29waWVzX1VubWFya2VkX0Rpc3RyaWJ1dGUvUjEtTWlzbGV2eSBldGFsICgyMDEyKS5wZGbSFwsYGVdOUy5kYXRhTxECQAAAAAACQAACAAAMTWFjaW50b3NoIEhEAAAAAAAAAAAAAAAAAAAAybeLVUgrAAACIbwqGlIxLU1pc2xldnkgZXRhbCAoMjAxMikucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIhvEDPPjaUAAAAAAAAAAAAAgAEAAAJIAAAAAAAAAAAAAAAAAAAABpDb3BpZXNfVW5tYXJrZWRfRGlzdHJpYnV0ZQAQAAgAAMm3w5UAAAARAAgAAM8+fOQAAAABABgCIbwqAiG8JQCmuWsAF59bAA1R2AAAmDoAAgB9TWFjaW50b3NoIEhEOlVzZXJzOgBhZ2FseWFyZHQ6AERyb3Bib3g6AFByaXZhdGUtRVJTSDg5OTA6AFJlYWRpbmdMaXN0OgBDb3BpZXNfVW5tYXJrZWRfRGlzdHJpYnV0ZToAUjEtTWlzbGV2eSBldGFsICgyMDEyKS5wZGYAAA4ANgAaAFIAMQAtAE0AaQBzAGwAZQB2AHkAIABlAHQAYQBsACAAKAAyADAAMQAyACkALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAGpVc2Vycy9hZ2FseWFyZHQvRHJvcGJveC9Qcml2YXRlLUVSU0g4OTkwL1JlYWRpbmdMaXN0L0NvcGllc19Vbm1hcmtlZF9EaXN0cmlidXRlL1IxLU1pc2xldnkgZXRhbCAoMjAxMikucGRmABMAAS8AABUAAgAQ//8AAIAG0hscHR5aJGNsYXNzbmFtZVgkY2xhc3Nlc11OU011dGFibGVEYXRhox0fIFZOU0RhdGFYTlNPYmplY3TSGxwiI1xOU0RpY3Rpb25hcnmiIiBfEA9OU0tleWVkQXJjaGl2ZXLRJidUcm9vdIABAAgAEQAaACMALQAyADcAQABGAE0AVQBgAGcAagBsAG4AcQBzAHUAdwCEAI4A6QDuAPYDOgM8A0EDTANVA2MDZwNuA3cDfAOJA4wDngOhA6YAAAAAAAACAQAAAAAAAAAoAAAAAAAAAAAAAAAAAAADqA==}}

@incollection{VanLehn08,
	Author = {Kurt VanLehn},
	Booktitle = {The future of assessment: Shaping teaching and learning.},
	Date-Added = {2014-03-27 14:52:29 +0000},
	Date-Modified = {2014-03-27 14:54:32 +0000},
	Editor = {C. Dwyer},
	Keywords = {intelligent tutoring systems},
	Pages = {113-138},
	Publisher = {Erbaum},
	Title = {Intelligent Tutoring Systems for Continuous, Embedded Assessment},
	Year = {2008},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8QXS4uLy4uL05ldyBBcnRpY2xlcy9SZWFkL1ZhbkxlaG4oMjAwOCktIEludGVsbGlnZW50IHR1dG9yaW5nIHN5c3RlbXMgZm9yIGNvbnRpbnVvdXMsIGVtYmVkLnBkZtIXCxgZV05TLmRhdGFPEQJgAAAAAAJgAAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADJt4tVSCsAAAE5yhcfVmFuTGVobigyMDA4KS0gSW50ZSMyMkY0MUFFLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAi9Brs9Zst0AAAAAAAAAAAACAAMAAAkgAAAAAAAAAAAAAAAAAAAABFJlYWQAEAAIAADJt8OVAAAAEQAIAADPWesdAAAAAQAUATnKFwAb8HgAF59bAA1R2AAAmDoAAgBbTWFjaW50b3NoIEhEOlVzZXJzOgBhZ2FseWFyZHQ6AERyb3Bib3g6AE5ldyBBcnRpY2xlczoAUmVhZDoAVmFuTGVobigyMDA4KS0gSW50ZSMyMkY0MUFFLnBkZgAADgCMAEUAVgBhAG4ATABlAGgAbgAoADIAMAAwADgAKQAtACAASQBuAHQAZQBsAGwAaQBnAGUAbgB0ACAAdAB1AHQAbwByAGkAbgBnACAAcwB5AHMAdABlAG0AcwAgAGYAbwByACAAYwBvAG4AdABpAG4AdQBvAHUAcwAsACAAZQBtAGIAZQBkAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgBvVXNlcnMvYWdhbHlhcmR0L0Ryb3Bib3gvTmV3IEFydGljbGVzL1JlYWQvVmFuTGVobigyMDA4KS0gSW50ZWxsaWdlbnQgdHV0b3Jpbmcgc3lzdGVtcyBmb3IgY29udGludW91cywgZW1iZWQucGRmAAATAAEvAAAVAAIAEP//AACABtIbHB0eWiRjbGFzc25hbWVYJGNsYXNzZXNdTlNNdXRhYmxlRGF0YaMdHyBWTlNEYXRhWE5TT2JqZWN00hscIiNcTlNEaWN0aW9uYXJ5oiIgXxAPTlNLZXllZEFyY2hpdmVy0SYnVHJvb3SAAQAIABEAGgAjAC0AMgA3AEAARgBNAFUAYABnAGoAbABuAHEAcwB1AHcAhACOAO4A8wD7A18DYQNmA3EDegOIA4wDkwOcA6EDrgOxA8MDxgPLAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAA80=}}

@article{Erosheva04,
	Author = {Elena Erosheva and Stephen E. Fienberg and John Lafferty},
	Date-Added = {2013-12-18 14:22:42 +0000},
	Date-Modified = {2013-12-18 14:25:57 +0000},
	Journal = {PNAS},
	Number = {Suppl.1},
	Pages = {5220-5227},
	Title = {Mixed-Membership Models of Scientific Publications},
	Volume = {101},
	Year = {2004},
	Bdsk-Url-1 = {http://www.pnas.org/cgi/content/full/101/suppl_1/5220}}

@incollection{Blei09,
	Author = {David Blei and John Lafferty},
	Booktitle = {Text Mining: Classification, Clustering, and Applications},
	Date-Added = {2013-12-18 14:09:12 +0000},
	Date-Modified = {2013-12-18 14:12:00 +0000},
	Editor = {A. Srivastava and M. Sahami},
	Publisher = {Chapman \& Hall/CRC},
	Series = {Data Mining and Knowledge Discovery Series},
	Title = {Topic Models},
	Year = {2009},
	Bdsk-Url-1 = {http://www.cs.princeton.edu/~blei/papers/BleiLafferty2009.pdf}}

@article{aleven_toward_2006,
	Author = {Aleven, V. and Mclaren, B. and Roll, I. and Koedinger, K.},
	Date-Added = {2013-12-05 22:14:42 +0000},
	Date-Modified = {2013-12-17 17:12:03 +0000},
	Issue = {2},
	Journal = {International Journal of Artificial Intelligence in Education},
	Pages = {101-128},
	Title = {Toward meta-cognitive tutoring: A model of help seeking with a Cognitive Tutor},
	Volume = {16},
	Year = {2006}}

@inproceedings{goldin_hints:_2013,
	Author = {Goldin, Ilya M. and Koedinger, Kenneth R. and Aleven, Vincent A. W. M. M.},
	Booktitle = {Proceedings of 6th International Conference on Educational Data Mining},
	Date-Added = {2013-12-05 22:14:42 +0000},
	Date-Modified = {2013-12-17 19:51:31 +0000},
	Editor = {{D'Mello}, Sidney K. and Calvo, Rafael A. and Olney, Andrew},
	Location = {Memphis, {TN}},
	Month = {July},
	Title = {Hints: You Can't Have Just One},
	Year = {2013},
	Bdsk-Url-1 = {http://www.educationaldatamining.org/EDM2013/papers/rn_paper_35.pdf}}

@inproceedings{goldin_learner_2012,
	Address = {Chania, Greece},
	Author = {Goldin, Ilya M. and Koedinger, Kenneth R. and Aleven, Vincent A. W. M. M.},
	Booktitle = {Proceedings of 5th International Conference on Educational Data Mining},
	Date-Added = {2013-12-05 22:14:42 +0000},
	Date-Modified = {2013-12-17 19:50:21 +0000},
	Editor = {Yacef, Kalina and Za{\"\i}ane, Osmar and Hershkovitz, Arnon and Yudelson, Michael and Stamper, John},
	Pages = {73-80},
	Title = {Learner Differences in Hint Processing},
	Year = {2012},
	Bdsk-Url-1 = {http://educationaldatamining.org/EDM2012/uploads/procs/Full_Papers/edm2012_full_6.pdf}}

@inproceedings{goldin_learner_2013,
	Address = {Memphis, {TN}},
	Author = {Goldin, Ilya M. and Carlson, Ryan},
	Booktitle = {Proceedings of 16th International Conference on Artificial Intelligence in Education},
	Date-Added = {2013-12-05 22:14:42 +0000},
	Date-Modified = {2013-12-17 17:14:16 +0000},
	Title = {Learner Differences and Hint Content},
	Year = {2013}}

@article{Girolami05,
	Abstract = {To provide a parsimonious generative representation of the sequential activity of a number of individuals within a population there is a necessary tradeoff between the definition of individual specific and global 
representations. A linear-time algorithm is proposed that defines a distributed predictive model for finite state symbolic sequences which represent the traces of the activity of a number of individuals within a group. The algorithm is based on a straightforward generalization of latent Dirichlet allocation to time-invariant Markov chains of arbitrary order. The modelling assumption made is that the possibly heterogeneous behavior of individuals may be represented by a relatively small number of simple and common behavioral traits which may interleave randomly according to an individual-specific distribution. The results of an empirical study on three different application domains indicate that this modelling approach provides an efficient low-complexity and intuitively interpretable representation scheme which is reflected by improved prediction performance over comparable models.},
	Annote = {Builds a mixed-membership model where the basis profiles are markov chains of varying order. 
Uses two types of estimation: Variational approximation, and "maximum a posteriori" (MAP)
Fits model on 3 different data sets of user-log data: actions in a word processor, sequences of calls to different geographic regions (UK), and finally web page browsing. 

Compares models on "perplexity" which they define as the exponential of the negative-normalized log-likelihood.  (Might be a standard markov-chain goodness of fit measure, or it might be more standard to ML)
They use t-tests and non-parametric Wilcoxon Rank-Sum tests to test for significant differences in perplexity.

They call their model a "simplical mixture of markov chains" and cite Minka and Lafferty 2002. 

For the zero^th order Markov model (when no memory is assumed in the markov process), then their model reduces to a multinomial LDA model. 

They also claim that Hofmann's pLSA algorithm is equivalent to LDA when the MAP estimator is calculated through an iterative convergence method. Suggests that LDA is an improvement over pLSA because of the estimation method used.  Cites Lappalainen and Miskin (2000) on the weakness of MAP estimators. 

For all the data sets, the mixed-membership model offers better results. Better on the goodness-of-fit measures and more interpretable results.  The differences were clearer on the bigger data sets with larger state-spaces. 

Data Structure:  only 1 item per model - the sequences (J=1)
	many replications of each item (big R) 

They offer some additional comments on computational algorithms and computational time.},
	Author = {Mark Girolami and Ata Kaban},
	Date-Added = {2013-06-25 19:41:00 +0000},
	Date-Modified = {2013-06-25 19:41:00 +0000},
	Journal = {Data Mining and Knowledge Discovery},
	Keywords = {Latent Dirichlet Allocation, markov chains, Mixture models, user profiling, mixed membership, variational approximation},
	Pages = {175-196},
	Title = {Sequential Activity Profiling: Latent Dirichlet Allocation of Markov Chains},
	Volume = {10},
	Year = {2005},
	Bdsk-File-1 = {YnBsaXN0MDDUAQIDBAUGJCVYJHZlcnNpb25YJG9iamVjdHNZJGFyY2hpdmVyVCR0b3ASAAGGoKgHCBMUFRYaIVUkbnVsbNMJCgsMDxJXTlMua2V5c1pOUy5vYmplY3RzViRjbGFzc6INDoACgAOiEBGABIAFgAdccmVsYXRpdmVQYXRoWWFsaWFzRGF0YV8Qay4uLy4uLy4uL0RvY3VtZW50cy9Kb3VybmFsLU5ld3MtQXJ0aWNsZXMvTWl4ZWQgTWVtYmVyc2hpcC9HaXJvbGFtaSAmIEthYmFuICgyMDA1KS0gTERBIG9mIG1hcmtvdiBjaGFpbnMucGRm0hcLGBlXTlMuZGF0YU8RAlwAAAAAAlwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAMtppx9IKwAAAAueSx9HaXJvbGFtaSAmIEthYmFuICgyMDAjQjlFNkEucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC55qyVXqE1BERiAAAAAAAAMABAAACSAAAAAAAAAAAAAAAAAAAAAQTWl4ZWQgTWVtYmVyc2hpcAAQAAgAAMtp7W8AAAARAAgAAMlWMGMAAAABABQAC55LAAucAgAFyJ8ABcieAAC5VwACAHJNYWNpbnRvc2ggSEQ6VXNlcnM6AGFnYWx5YXJkdDoARG9jdW1lbnRzOgBKb3VybmFsLU5ld3MtQXJ0aWNsZXM6AE1peGVkIE1lbWJlcnNoaXA6AEdpcm9sYW1pICYgS2FiYW4gKDIwMCNCOUU2QS5wZGYADgBkADEARwBpAHIAbwBsAGEAbQBpACAAJgAgAEsAYQBiAGEAbgAgACgAMgAwADAANQApAC0AIABMAEQAQQAgAG8AZgAgAG0AYQByAGsAbwB2ACAAYwBoAGEAaQBuAHMALgBwAGQAZgAPABoADABNAGEAYwBpAG4AdABvAHMAaAAgAEgARAASAHJVc2Vycy9hZ2FseWFyZHQvRG9jdW1lbnRzL0pvdXJuYWwtTmV3cy1BcnRpY2xlcy9NaXhlZCBNZW1iZXJzaGlwL0dpcm9sYW1pICYgS2FiYW4gKDIwMDUpLSBMREEgb2YgbWFya292IGNoYWlucy5wZGYAEwABLwAAFQACABD//wAAgAbSGxwdHlokY2xhc3NuYW1lWCRjbGFzc2VzXU5TTXV0YWJsZURhdGGjHR8gVk5TRGF0YVhOU09iamVjdNIbHCIjXE5TRGljdGlvbmFyeaIiIF8QD05TS2V5ZWRBcmNoaXZlctEmJ1Ryb290gAEACAARABoAIwAtADIANwBAAEYATQBVAGAAZwBqAGwAbgBxAHMAdQB3AIQAjgD8AQEBCQNpA2sDcAN7A4QDkgOWA50DpgOrA7gDuwPNA9AD1QAAAAAAAAIBAAAAAAAAACgAAAAAAAAAAAAAAAAAAAPX}}
